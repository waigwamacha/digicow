{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior rows: 44882\n",
      "Train rows: 13536\n",
      "Exact session overlaps (farmer + date): 0\n",
      "Overlap as % of Train: 0.0%\n",
      "=== TRAINERS ===\n",
      "Prior: {'TRA_hyodnntj', 'TRA_ubcgvofe', 'TRA_szrwyfzz', 'TRA_rkvyofbh', 'TRA_gertumxc', 'TRA_kkzpfdtu', 'TRA_dttdgplk', 'TRA_twwcfcum', 'TRA_suiifsur'}\n",
      "Train: {'TRA_hyodnntj', 'TRA_ubcgvofe', 'TRA_szrwyfzz', 'TRA_rkvyofbh', 'TRA_gertumxc', 'TRA_kkzpfdtu', 'TRA_dttdgplk', 'TRA_twwcfcum', 'TRA_suiifsur'}\n",
      "Overlap: {'TRA_hyodnntj', 'TRA_ubcgvofe', 'TRA_szrwyfzz', 'TRA_rkvyofbh', 'TRA_gertumxc', 'TRA_kkzpfdtu', 'TRA_dttdgplk', 'TRA_twwcfcum', 'TRA_suiifsur'}\n",
      "\n",
      "=== DATES ===\n",
      "Prior:  2024-01-03 → 2025-12-11\n",
      "Train:  2024-01-03 → 2025-04-12\n",
      "Test:   2025-05-02  → 2025-12-12\n",
      "Prior rows overlapping test period (>= 2025-05-02): 6,546\n",
      "\n",
      "=== ADOPTION RATES ===\n",
      "  adopted_within_07_days: Prior=0.0148  Train=0.0113\n",
      "  adopted_within_90_days: Prior=0.0340  Train=0.0158\n",
      "  adopted_within_120_days: Prior=0.0465  Train=0.0223\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score, log_loss, average_precision_score\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "# import xgboost as xgb\n",
    "import ast\n",
    "\n",
    "# ── 1. Load ──────────────────────────────────────────────────────────────────\n",
    "# train_df = pd.read_csv('Train.csv')\n",
    "# test_df  = pd.read_csv('Test.csv')\n",
    "train_df = pd.read_csv('/app/digicow/data/Train.csv')\n",
    "prior_df = pd.read_csv('/app/digicow/data/Prior.csv')\n",
    "test_df = pd.read_csv('/app/digicow/data/Test.csv')\n",
    "\n",
    "prior_df['dt'] = pd.to_datetime(prior_df['training_day'])\n",
    "train_df['dt'] = pd.to_datetime(train_df['training_day'])\n",
    "test_df['dt']  = pd.to_datetime(test_df['training_day'])\n",
    "\n",
    "# ── Overlap check ──────────────────────────────────────────\n",
    "# Check overlap between Prior and Train sessions\n",
    "overlap = prior_df.merge(\n",
    "    train_df[['farmer_name', 'training_day']],\n",
    "    on=['farmer_name', 'training_day']\n",
    ")\n",
    "print(f\"Prior rows: {len(prior_df)}\")\n",
    "print(f\"Train rows: {len(train_df)}\")\n",
    "print(f\"Exact session overlaps (farmer + date): {len(overlap)}\")\n",
    "print(f\"Overlap as % of Train: {len(overlap)/len(train_df)*100:.1f}%\")\n",
    "\n",
    "# 1. Trainer overlap\n",
    "# prior_trainers = set(prior_df.trainer.dropna())\n",
    "# train_trainers = set(train_df.trainer.dropna())\n",
    "import ast\n",
    "\n",
    "# def parse_trainer(s):\n",
    "#     try:\n",
    "#         parsed = ast.literal_eval(s)\n",
    "#         if isinstance(parsed, list):\n",
    "#             return parsed[0] if len(parsed) == 1 else str(parsed)\n",
    "#         return s\n",
    "#     except:\n",
    "#         return s\n",
    "def parse_trainer(s):\n",
    "    try:\n",
    "        parsed = ast.literal_eval(s)\n",
    "        if isinstance(parsed, list):\n",
    "            return parsed[0]  # always take first trainer, even for multi-trainer rows\n",
    "        return s\n",
    "    except:\n",
    "        return s\n",
    "train_df['trainer'] = train_df['trainer'].apply(parse_trainer)\n",
    "test_df['trainer']  = test_df['trainer'].apply(parse_trainer)\n",
    "\n",
    "# Recompute AFTER parsing\n",
    "prior_trainers = set(prior_df.trainer.dropna())\n",
    "train_trainers = set(train_df.trainer.dropna())\n",
    "\n",
    "print(\"=== TRAINERS ===\")\n",
    "print(f\"Prior: {prior_trainers}\")\n",
    "print(f\"Train: {train_trainers}\")\n",
    "print(f\"Overlap: {prior_trainers & train_trainers}\")\n",
    "\n",
    "\n",
    "# 2. Date ranges\n",
    "print(\"\\n=== DATES ===\")\n",
    "print(f\"Prior:  {prior_df.dt.min().date()} → {prior_df.dt.max().date()}\")\n",
    "print(f\"Train:  {train_df.dt.min().date()} → {train_df.dt.max().date()}\")\n",
    "print(f\"Test:   {test_df.dt.min().date()}  → {test_df.dt.max().date()}\")\n",
    "print(f\"Prior rows overlapping test period (>= 2025-05-02): {(prior_df.dt >= '2025-05-02').sum():,}\")\n",
    "\n",
    "# 3. Adoption rates\n",
    "print(\"\\n=== ADOPTION RATES ===\")\n",
    "for c in ['adopted_within_07_days','adopted_within_90_days','adopted_within_120_days']:\n",
    "    print(f\"  {c}: Prior={prior_df[c].mean():.4f}  Train={train_df[c].mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train unique farmers: 13536 / total rows: 13536\n",
      "\n",
      "Prior unique farmers: 6719 / total rows: 44882\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>training_day</th>\n",
       "      <th>topics_list</th>\n",
       "      <th>adopted_within_07_days</th>\n",
       "      <th>adopted_within_90_days</th>\n",
       "      <th>adopted_within_120_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4690</th>\n",
       "      <td>2024-02-07</td>\n",
       "      <td>['How To Rear A Calf With Unga Products']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4691</th>\n",
       "      <td>2024-02-07</td>\n",
       "      <td>['How To Rear A Calf With Unga Products']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4692</th>\n",
       "      <td>2024-02-07</td>\n",
       "      <td>['How To Rear A Calf With Unga Products']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4693</th>\n",
       "      <td>2024-02-07</td>\n",
       "      <td>['How To Rear A Calf With Unga Products']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4694</th>\n",
       "      <td>2024-02-07</td>\n",
       "      <td>['Transition Cow Management- Care For Your Cow...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40225</th>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>['Importance Of Vaccinations And Record']</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40226</th>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>['Importance Of Vaccinating Against East Coast...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40227</th>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>['Importance Of Vaccinating Against East Coast...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40228</th>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>['Importance Of Vaccinating Against East Coast...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40229</th>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>['Importance Of Vaccinating Against East Coast...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      training_day                                        topics_list  \\\n",
       "4690    2024-02-07          ['How To Rear A Calf With Unga Products']   \n",
       "4691    2024-02-07          ['How To Rear A Calf With Unga Products']   \n",
       "4692    2024-02-07          ['How To Rear A Calf With Unga Products']   \n",
       "4693    2024-02-07          ['How To Rear A Calf With Unga Products']   \n",
       "4694    2024-02-07  ['Transition Cow Management- Care For Your Cow...   \n",
       "...            ...                                                ...   \n",
       "40225   2025-06-05          ['Importance Of Vaccinations And Record']   \n",
       "40226   2025-06-05  ['Importance Of Vaccinating Against East Coast...   \n",
       "40227   2025-06-05  ['Importance Of Vaccinating Against East Coast...   \n",
       "40228   2025-06-05  ['Importance Of Vaccinating Against East Coast...   \n",
       "40229   2025-06-05  ['Importance Of Vaccinating Against East Coast...   \n",
       "\n",
       "       adopted_within_07_days  adopted_within_90_days  adopted_within_120_days  \n",
       "4690                        0                       0                        0  \n",
       "4691                        0                       0                        0  \n",
       "4692                        0                       0                        0  \n",
       "4693                        0                       0                        0  \n",
       "4694                        0                       0                        0  \n",
       "...                       ...                     ...                      ...  \n",
       "40225                       0                       1                        1  \n",
       "40226                       0                       0                        0  \n",
       "40227                       0                       0                        0  \n",
       "40228                       0                       0                        0  \n",
       "40229                       0                       1                        1  \n",
       "\n",
       "[96 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Groups with >1 row (true duplicates): 3330\n",
      "Total groups: 39674\n",
      "Groups with >1 row: 3330\n",
      "% duplicated: 8.39\n",
      "  Conflicting adopted_within_07_days: 266\n",
      "  Conflicting adopted_within_90_days: 429\n",
      "  Conflicting adopted_within_120_days: 560\n"
     ]
    }
   ],
   "source": [
    "# # unique farmers\n",
    "# 1. Train has no repeat farmers\n",
    "print(\"Train unique farmers:\", train_df['farmer_name'].nunique(), \"/ total rows:\", len(train_df))\n",
    "\n",
    "# # 2. Prior has repeat farmers (multiple sessions)\n",
    "print(\"\\nPrior unique farmers:\", prior_df['farmer_name'].nunique(), \"/ total rows:\", len(prior_df))\n",
    "\n",
    "# # 3. topics_list + training_day explain most repeats — show one farmer\n",
    "top_farmer = prior_df['farmer_name'].value_counts().index[0]\n",
    "cols = ['training_day', 'topics_list', 'adopted_within_07_days', 'adopted_within_90_days', 'adopted_within_120_days']\n",
    "display(prior_df[prior_df['farmer_name'] == top_farmer][cols])\n",
    "\n",
    "prior_df.groupby(['farmer_name', 'training_day', 'topics_list']).size().value_counts()\n",
    "# So the \"4 people in a group session\" theory doesn't hold cleanly. It's more likely just noisy data collection — same record entered multiple times with slight variations.\n",
    "\n",
    "# # 4. True duplicates: same farmer + date + topic, still multiple rows\n",
    "group_cols = ['farmer_name', 'training_day', 'topics_list']\n",
    "grp_sizes = prior_df.groupby(group_cols).size()\n",
    "print(\"\\nGroups with >1 row (true duplicates):\", (grp_sizes > 1).sum())\n",
    "\n",
    "print(\"Total groups:\", len(grp_sizes))\n",
    "print(\"Groups with >1 row:\", (grp_sizes > 1).sum())\n",
    "print(\"% duplicated:\", round((grp_sizes > 1).sum() / len(grp_sizes) * 100, 2))\n",
    "\n",
    "# # 5. How many of those have conflicting labels?\n",
    "for col in ['adopted_within_07_days', 'adopted_within_90_days', 'adopted_within_120_days']:\n",
    "    n = (prior_df.groupby(group_cols)[col].nunique() > 1).sum()\n",
    "    print(f\"  Conflicting {col}: {n}\")\n",
    "\n",
    "# # # 6. Test duplicates (ignoring ID)\n",
    "# feat_cols = [c for c in test_df.columns if c != 'ID']\n",
    "# print(\"\\nTest duplicate rows (ignoring ID):\", test_df.duplicated(subset=feat_cols).sum(), \"/ total:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train farmers with prior history: 3193 / 13536\n",
      "Test farmers with prior history:  3526 / 5621\n",
      "Train farmers appearing in Prior: 3193 / 13536\n"
     ]
    }
   ],
   "source": [
    "# PRIOR: \n",
    "# \n",
    "# 1. Aggregate duplicates by mean (handles both consistent + conflicting)\n",
    "prior_clean = prior_df.groupby(['farmer_name', 'training_day', 'topics_list']).agg({\n",
    "    'adopted_within_07_days': 'mean',\n",
    "    'adopted_within_90_days': 'mean', \n",
    "    'adopted_within_120_days': 'mean',\n",
    "    # keep other cols\n",
    "    'county': 'first', 'gender': 'first', 'age': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# 2. Parse topics and explode\n",
    "prior_clean['topics_parsed'] = prior_clean['topics_list'].apply(ast.literal_eval)\n",
    "prior_clean['training_day'] = pd.to_datetime(prior_clean['training_day'])\n",
    "prior_exploded = prior_clean.explode('topics_parsed')\n",
    "\n",
    "# 3a. For TRAIN — only use Prior history before each row's training date (avoid leakage)\n",
    "train_df['training_day'] = pd.to_datetime(train_df['training_day'])\n",
    "\n",
    "prior_for_train = prior_exploded.merge(\n",
    "    train_df[['farmer_name', 'training_day']].rename(columns={'training_day': 'current_date'}),\n",
    "    on='farmer_name', how='inner'\n",
    ")\n",
    "prior_for_train = prior_for_train[prior_for_train['training_day'] < prior_for_train['current_date']]\n",
    "\n",
    "farmer_history_train = prior_for_train.groupby('farmer_name').agg(\n",
    "    n_prior_sessions    = ('training_day', 'nunique'),\n",
    "    n_prior_topics      = ('topics_parsed', 'nunique'),\n",
    "    hist_adopt_rate_07  = ('adopted_within_07_days', 'mean'),\n",
    "    hist_adopt_rate_90  = ('adopted_within_90_days', 'mean'),\n",
    "    hist_adopt_rate_120 = ('adopted_within_120_days', 'mean'),\n",
    "    last_training_day   = ('training_day', 'max'),\n",
    ").reset_index()\n",
    "\n",
    "train_df = train_df.merge(farmer_history_train, on='farmer_name', how='left')\n",
    "\n",
    "\n",
    "# 3b. For TEST — do same for test\n",
    "test_df['training_day'] = pd.to_datetime(test_df['training_day'])\n",
    "\n",
    "prior_for_test = prior_exploded.merge(\n",
    "    test_df[['farmer_name', 'training_day']].rename(columns={'training_day': 'current_date'}),\n",
    "    on='farmer_name', how='inner'\n",
    ")\n",
    "prior_for_test = prior_for_test[prior_for_test['training_day'] < prior_for_test['current_date']]\n",
    "\n",
    "farmer_history_test = prior_for_test.groupby('farmer_name').agg(\n",
    "    n_prior_sessions    = ('training_day', 'nunique'),\n",
    "    n_prior_topics      = ('topics_parsed', 'nunique'),\n",
    "    hist_adopt_rate_07  = ('adopted_within_07_days', 'mean'),\n",
    "    hist_adopt_rate_90  = ('adopted_within_90_days', 'mean'),\n",
    "    hist_adopt_rate_120 = ('adopted_within_120_days', 'mean'),\n",
    "    last_training_day   = ('training_day', 'max'),\n",
    ").reset_index()\n",
    "\n",
    "test_df = test_df.merge(farmer_history_test, on='farmer_name', how='left')\n",
    "\n",
    "\n",
    "\n",
    "history_cols = ['n_prior_sessions', 'n_prior_topics', 'hist_adopt_rate_07', \n",
    "                'hist_adopt_rate_90', 'hist_adopt_rate_120']\n",
    "\n",
    "train_df[history_cols] = train_df[history_cols].fillna(0)\n",
    "test_df[history_cols]  = test_df[history_cols].fillna(0)\n",
    "\n",
    "# last_training_day NaN means no history — fill with a sentinel or leave for recency feature later\n",
    "# Recency feature\n",
    "train_df['days_since_last_training'] = (\n",
    "    train_df['training_day'] - pd.to_datetime(train_df['last_training_day'])\n",
    ").dt.days.fillna(9999)\n",
    "\n",
    "test_df['days_since_last_training'] = (\n",
    "    test_df['training_day'] - pd.to_datetime(test_df['last_training_day'])\n",
    ").dt.days.fillna(9999)\n",
    "\n",
    "train_df.drop(columns=['last_training_day'], inplace=True)\n",
    "test_df.drop(columns=['last_training_day'], inplace=True)\n",
    "\n",
    "print(\"Train farmers with prior history:\", train_df['n_prior_sessions'].gt(0).sum(), '/', len(train_df))\n",
    "print(\"Test farmers with prior history: \", test_df['n_prior_sessions'].gt(0).sum(), '/', len(test_df))\n",
    "\n",
    "train_farmers_in_prior = train_df['farmer_name'].isin(prior_df['farmer_name'])\n",
    "print(\"Train farmers appearing in Prior:\", train_farmers_in_prior.sum(), '/', len(train_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-11 2024-01-03 00:00:00\n",
      "Prior range: 2024-01-03 → 2025-12-11\n",
      "Train range: 2024-01-03 00:00:00 → 2025-04-12 00:00:00\n",
      "Test range: 2025-05-02 00:00:00 → 2025-12-12 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# train_df.columns\n",
    "print(prior_df['training_day'].max(), train_df['training_day'].min())\n",
    "# Check date overlap between Prior and Train/Test\n",
    "print(\"Prior range:\", prior_df['training_day'].min(), \"→\", prior_df['training_day'].max())\n",
    "print(\"Train range:\", train_df['training_day'].min(), \"→\", train_df['training_day'].max())\n",
    "print(\"Test range:\", test_df['training_day'].min(), \"→\", test_df['training_day'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['Importance Of Vaccination'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score, log_loss, average_precision_score\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "\n",
    "\n",
    "# ── Group-level adoption rates from Prior ─────────────────────────────────────\n",
    "# for grp in ['trainer', 'county', 'subcounty', 'ward']:\n",
    "#     for days in ['07', '90', '120']:\n",
    "#         col = f'adopted_within_{days}_days'\n",
    "#         rate_map = prior_df.groupby(grp)[col].mean()\n",
    "#         feat_name = f'{grp}_prior_adopt_{days}d'\n",
    "#         global_mean = prior_df[col].mean()\n",
    "#         train_df[feat_name] = train_df[grp].map(rate_map).fillna(global_mean)\n",
    "#         test_df[feat_name]  = test_df[grp].map(rate_map).fillna(global_mean)\n",
    "\n",
    "# group_rate_cols = [f'{grp}_prior_adopt_{days}d' \n",
    "#                    for grp in ['trainer', 'county', 'subcounty', 'ward']\n",
    "#                    for days in ['07', '90', '120']]\n",
    "\n",
    "for grp in ['trainer', 'county', 'subcounty', 'ward', 'group_name']:  # ← add group_name\n",
    "    for days in ['07', '90', '120']:\n",
    "        col = f'adopted_within_{days}_days'\n",
    "        rate_map = prior_df.groupby(grp)[col].mean()\n",
    "        feat_name = f'{grp}_prior_adopt_{days}d'\n",
    "        global_mean = prior_df[col].mean()\n",
    "        train_df[feat_name] = train_df[grp].map(rate_map).fillna(global_mean)\n",
    "        test_df[feat_name]  = test_df[grp].map(rate_map).fillna(global_mean)\n",
    "\n",
    "group_rate_cols = [f'{grp}_prior_adopt_{days}d' \n",
    "                   for grp in ['trainer', 'county', 'subcounty', 'ward', 'group_name']  # ← here too\n",
    "                   for days in ['07', '90', '120']]\n",
    "\n",
    "\n",
    "# ── 2. Parse topics ──────────────────────────────────────────────────────────\n",
    "def parse_topics(s):\n",
    "    try:\n",
    "        parsed = ast.literal_eval(s)\n",
    "        if isinstance(parsed, list):\n",
    "            flat = []\n",
    "            for item in parsed:\n",
    "                if isinstance(item, list): flat.extend(item)\n",
    "                else: flat.append(item)\n",
    "            return flat\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "topics_train = pd.DataFrame(\n",
    "    mlb.fit_transform(train_df['topics_list'].apply(parse_topics)),\n",
    "    columns=mlb.classes_, index=train_df.index\n",
    ")\n",
    "topics_test = pd.DataFrame(\n",
    "    mlb.transform(test_df['topics_list'].apply(parse_topics)),\n",
    "    columns=mlb.classes_, index=test_df.index\n",
    ")\n",
    "train_df = pd.concat([train_df, topics_train], axis=1).drop(columns=['topics_list'])\n",
    "test_df  = pd.concat([test_df,  topics_test],  axis=1).drop(columns=['topics_list'])\n",
    "\n",
    "# ['Importance Of Vaccination'] how common is it in test? as it doesnt appear in train\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 11,318 | Val: 2,218\n",
      "Train: 11,318 | Val: 2,218\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ── Temporal split ────────────────────────────────────────────────────────────\n",
    "train_df['dt'] = pd.to_datetime(train_df['training_day'])\n",
    "test_df['dt']  = pd.to_datetime(test_df['training_day'])\n",
    "\n",
    "cutoff = pd.Timestamp('2025-01-01')\n",
    "df_val = train_df[train_df['dt'] >= cutoff].copy()\n",
    "df_tr  = train_df[train_df['dt'] <  cutoff].copy()\n",
    "print(f\"Train: {len(df_tr):,} | Val: {len(df_val):,}\")\n",
    "\n",
    "print(f\"Train: {len(df_tr):,} | Val: {len(df_val):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Features ──────────────────────────────────────────────────────────────────\n",
    "cat_cols     = ['gender', 'registration', 'age', 'trainer', 'county', 'subcounty', 'ward']\n",
    "topic_cols   = list(mlb.classes_)\n",
    "# numeric_cols = ['belong_to_cooperative', 'has_topic_trained_on', #'topic_overlap_frac',\n",
    "#                 'n_prior_sessions', 'n_prior_topics',\n",
    "#                 'hist_adopt_rate_07', 'hist_adopt_rate_90', 'hist_adopt_rate_120',\n",
    "#                 'days_since_last_training'] + topic_cols\n",
    "numeric_cols = ['belong_to_cooperative', 'has_topic_trained_on',\n",
    "                'n_prior_sessions', 'n_prior_topics',\n",
    "                'hist_adopt_rate_07', 'hist_adopt_rate_90', 'hist_adopt_rate_120',\n",
    "                'days_since_last_training'] + group_rate_cols + topic_cols\n",
    "feature_cols = cat_cols + numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'07': {'lgb': {'learning_rate': 0.01277588617013218, 'num_leaves': 69, 'min_child_samples': 66, 'feature_fraction': 0.7931799823039588, 'bagging_fraction': 0.38580794509945804, 'bagging_freq': 10, 'reg_alpha': 0.0004814490654559664, 'reg_lambda': 0.1359271205257805}, 'cat': {'learning_rate': 0.05192523263427517, 'depth': 6, 'l2_leaf_reg': 8.210394746047953, 'bagging_temperature': 0.031073459271315294, 'random_strength': 1.3764590239003986, 'border_count': 67}}, '90': {'lgb': {'learning_rate': 0.019998939263562214, 'num_leaves': 54, 'min_child_samples': 12, 'feature_fraction': 0.8686223981720478, 'bagging_fraction': 0.7275940125211089, 'bagging_freq': 7, 'reg_alpha': 0.0014867695769971536, 'reg_lambda': 0.011031682276024198}, 'cat': {'learning_rate': 0.062419052252125025, 'depth': 7, 'l2_leaf_reg': 1.29008959776147, 'bagging_temperature': 0.027956026956829527, 'random_strength': 0.914834670069054, 'border_count': 59}}, '120': {'lgb': {'learning_rate': 0.012744638435231487, 'num_leaves': 91, 'min_child_samples': 11, 'feature_fraction': 0.7469437729260306, 'bagging_fraction': 0.6374533711103695, 'bagging_freq': 3, 'reg_alpha': 0.00035422038066651446, 'reg_lambda': 0.008018799717827964}, 'cat': {'learning_rate': 0.06153162991791282, 'depth': 5, 'l2_leaf_reg': 13.007627157935254, 'bagging_temperature': 1.377857238474781, 'random_strength': 1.9900234693842744, 'border_count': 247}}} {'07': 0.552026240075519, '90': 0.645073102444733, '120': 0.7504657754251228}\n",
      "[LGB 07d]  AUC=0.8938  LogLoss=0.0610  PR-AUC=0.2029\n",
      "[CAT 07d]  AUC=0.9259  LogLoss=0.0611  PR-AUC=0.1414\n",
      "[ENS 07d]  AUC=0.9232  LogLoss=0.0595  PR-AUC=0.1923  CAT_w=0.552\n",
      "\n",
      "[LGB 90d]  AUC=0.9171  LogLoss=0.0617  PR-AUC=0.1289\n",
      "[CAT 90d]  AUC=0.9108  LogLoss=0.0597  PR-AUC=0.1541\n",
      "[ENS 90d]  AUC=0.9238  LogLoss=0.0579  PR-AUC=0.1563  CAT_w=0.645\n",
      "\n",
      "[LGB 120d]  AUC=0.9071  LogLoss=0.0717  PR-AUC=0.1383\n",
      "[CAT 120d]  AUC=0.9087  LogLoss=0.0666  PR-AUC=0.1343\n",
      "[ENS 120d]  AUC=0.9127  LogLoss=0.0666  PR-AUC=0.1376  CAT_w=0.750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# w/tuning\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "import json\n",
    "\n",
    "\n",
    "# LightGBM needs categoricals as 'category' dtype\n",
    "def prep_lgb(df):\n",
    "    X = df[feature_cols].copy()\n",
    "    for c in cat_cols:\n",
    "        X[c] = X[c].astype('category')\n",
    "    return X\n",
    "\n",
    "X_tr_lgb   = prep_lgb(df_tr)\n",
    "X_val_lgb  = prep_lgb(df_val)\n",
    "X_test_lgb = prep_lgb(test_df)\n",
    "\n",
    "# CatBoost takes raw strings — no encoding needed\n",
    "X_tr_cat   = df_tr[feature_cols].fillna('missing')\n",
    "X_val_cat  = df_val[feature_cols].fillna('missing')\n",
    "X_test_cat = test_df[feature_cols].fillna('missing')\n",
    "\n",
    "cat_feature_indices = [feature_cols.index(c) for c in cat_cols]\n",
    "\n",
    "targets = ['07', '90', '120']\n",
    "lgb_preds  = {}\n",
    "cat_preds  = {}\n",
    "xgb_preds = {}\n",
    "ens_preds  = {}\n",
    "\n",
    "# # # ── Tuning cell (run once, save results) ─────────────────────────────────────\n",
    "# def optimize_weight(cat_prob, lgb_prob, y_true):\n",
    "#     def objective(trial):\n",
    "#         w = trial.suggest_float('cat_weight', 0.0, 1.0)\n",
    "#         return log_loss(y_true, w * cat_prob + (1-w) * lgb_prob)\n",
    "#     # study = optuna.create_study(direction='minimize')\n",
    "#     study = optuna.create_study(\n",
    "#         direction='minimize',\n",
    "#         sampler=optuna.samplers.TPESampler(n_startup_trials=20, seed=42)\n",
    "#     )\n",
    "#     study.optimize(objective, n_trials=100)\n",
    "#     return study.best_params['cat_weight']\n",
    "\n",
    "# def tune_lgb(X_tr, y_tr, X_val, y_val, scale_pos):\n",
    "#     def objective(trial):\n",
    "#         params = {\n",
    "#             'objective': 'binary', 'metric': 'binary_logloss',\n",
    "#             'scale_pos_weight': scale_pos, 'verbose': -1, 'seed': 42,\n",
    "#             'learning_rate':     trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "#             'num_leaves':        trial.suggest_int('num_leaves', 16, 128),\n",
    "#             'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "#             'feature_fraction':  trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "#             'bagging_fraction':  trial.suggest_float('bagging_fraction', 0.3, 1.0),\n",
    "#             'bagging_freq':      trial.suggest_int('bagging_freq', 1, 10),\n",
    "#             'reg_alpha':         trial.suggest_float('reg_alpha', 1e-4, 10.0, log=True),\n",
    "#             'reg_lambda':        trial.suggest_float('reg_lambda', 1e-4, 10.0, log=True),\n",
    "#         }\n",
    "#         ds_tr  = lgb.Dataset(X_tr, label=y_tr)\n",
    "#         ds_val = lgb.Dataset(X_val, label=y_val, reference=ds_tr)\n",
    "#         model  = lgb.train(params, ds_tr, num_boost_round=500, valid_sets=[ds_val],\n",
    "#                            callbacks=[lgb.early_stopping(50, verbose=False), lgb.log_evaluation(0)])\n",
    "#         return log_loss(y_val, model.predict(X_val))\n",
    "#     # study = optuna.create_study(direction='minimize')\n",
    "#     study = optuna.create_study(\n",
    "#         direction='minimize',\n",
    "#         sampler=optuna.samplers.TPESampler(n_startup_trials=20, seed=42)\n",
    "#     )\n",
    "#     study.optimize(objective, n_trials=100, show_progress_bar=True)\n",
    "#     return study.best_params\n",
    "\n",
    "# def tune_cat(X_tr, y_tr, X_val, y_val, cat_feature_indices):\n",
    "#     def objective(trial):\n",
    "#         model = CatBoostClassifier(\n",
    "#             iterations=2000, random_seed=42, verbose=0,\n",
    "#             cat_features=cat_feature_indices, early_stopping_rounds=50,\n",
    "#             loss_function='Logloss', eval_metric='Logloss',\n",
    "#             learning_rate=       trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "#             depth=               trial.suggest_int('depth', 4, 8),\n",
    "#             l2_leaf_reg=         trial.suggest_float('l2_leaf_reg', 1.0, 20.0),\n",
    "#             bagging_temperature= trial.suggest_float('bagging_temperature', 0.0, 2.0),\n",
    "#             random_strength=     trial.suggest_float('random_strength', 0.0, 2.0),\n",
    "#             border_count=        trial.suggest_int('border_count', 32, 255),\n",
    "#         )\n",
    "#         model.fit(X_tr, y_tr, eval_set=(X_val, y_val))\n",
    "#         return log_loss(y_val, model.predict_proba(X_val)[:, 1])\n",
    "#     # study = optuna.create_study(direction='minimize')\n",
    "#     study = optuna.create_study(\n",
    "#         direction='minimize',\n",
    "#         sampler=optuna.samplers.TPESampler(n_startup_trials=20, seed=42)\n",
    "#     )\n",
    "#     study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "#     return study.best_params\n",
    "\n",
    "# best_params  = {}\n",
    "# best_weights = {}\n",
    "\n",
    "# for days in targets:\n",
    "#     col = f'adopted_within_{days}_days'\n",
    "#     y_tr  = df_tr[col].values\n",
    "#     y_val = df_val[col].values\n",
    "#     prevalence = y_tr.mean()\n",
    "#     scale_pos  = (1 - prevalence) / prevalence\n",
    "\n",
    "#     print(f\"\\n── Tuning {days}d ──\")\n",
    "#     lgb_best = tune_lgb(X_tr_lgb, y_tr, X_val_lgb, y_val, scale_pos)\n",
    "#     cat_best = tune_cat(X_tr_cat, y_tr, X_val_cat, y_val, cat_feature_indices)\n",
    "#     best_params[days] = {'lgb': lgb_best, 'cat': cat_best}\n",
    "\n",
    "#     # Retrain with best params to get val probs for weight tuning\n",
    "#     ds_tr  = lgb.Dataset(X_tr_lgb, label=y_tr)\n",
    "#     ds_val = lgb.Dataset(X_val_lgb, label=y_val, reference=ds_tr)\n",
    "#     lgb_model = lgb.train(\n",
    "#         {'objective':'binary','metric':'binary_logloss','scale_pos_weight':scale_pos,\n",
    "#          'verbose':-1,'seed':42, **lgb_best},\n",
    "#         ds_tr, num_boost_round=500, valid_sets=[ds_val],\n",
    "#         callbacks=[lgb.early_stopping(50, verbose=False), lgb.log_evaluation(0)]\n",
    "#     )\n",
    "#     cat_model = CatBoostClassifier(\n",
    "#         iterations=2000, loss_function='Logloss', eval_metric='Logloss',\n",
    "#         cat_features=cat_feature_indices, early_stopping_rounds=50,\n",
    "#         random_seed=42, verbose=0, **cat_best\n",
    "#     )\n",
    "#     cat_model.fit(X_tr_cat, y_tr, eval_set=(X_val_cat, y_val))\n",
    "\n",
    "#     # best_w = optimize_weight(cat_model.predict_proba(X_val_cat)[:,1], lgb_model.predict(X_val_lgb), y_val)\n",
    "#     lgb_val_prob_tune = lgb_model.predict(X_val_lgb)\n",
    "#     cat_val_prob_tune = cat_model.predict_proba(X_val_cat)[:, 1]\n",
    "#     print(f\"  LGB  AUC={roc_auc_score(y_val, lgb_val_prob_tune):.4f}  LL={log_loss(y_val, lgb_val_prob_tune):.4f}\")\n",
    "#     print(f\"  CAT  AUC={roc_auc_score(y_val, cat_val_prob_tune):.4f}  LL={log_loss(y_val, cat_val_prob_tune):.4f}\")\n",
    "#     best_w = optimize_weight(cat_val_prob_tune, lgb_val_prob_tune, y_val)\n",
    "#     best_weights[days] = best_w\n",
    "#     print(f\"{days}d → LGB: {lgb_best} | CAT: {cat_best} | CAT weight: {best_w:.3f}\")\n",
    "\n",
    "# with open('/app/digicow/best_params.json', 'w') as f:\n",
    "#     json.dump({'params': best_params, 'weights': best_weights}, f, indent=2)\n",
    "# print(\"Saved best_params.json\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open('/app/digicow/best_params.json') as f:\n",
    "    saved = json.load(f)\n",
    "best_params  = saved['params']\n",
    "best_weights = saved['weights']\n",
    "print(best_params, best_weights)\n",
    "\n",
    "# ── 5. Train per target (with tuned params) ───────────────────────────────────\n",
    "for days in targets:\n",
    "    col = f'adopted_within_{days}_days'\n",
    "    y_tr  = df_tr[col].values\n",
    "    y_val = df_val[col].values\n",
    "    prevalence = y_tr.mean()\n",
    "    scale_pos  = (1 - prevalence) / prevalence\n",
    "\n",
    "    # ── LightGBM ──\n",
    "    lgb_tr  = lgb.Dataset(X_tr_lgb, label=y_tr)\n",
    "    lgb_val_ds = lgb.Dataset(X_val_lgb, label=y_val, reference=lgb_tr)\n",
    "    params = {\n",
    "        'objective': 'binary', 'metric': ['binary_logloss', 'auc'],\n",
    "        'scale_pos_weight': scale_pos, 'verbose': -1, 'seed': 42,\n",
    "        **best_params[days]['lgb']\n",
    "    }\n",
    "    lgb_model = lgb.train(params, lgb_tr, num_boost_round=500,\n",
    "        valid_sets=[lgb_val_ds],\n",
    "        callbacks=[lgb.early_stopping(50, verbose=False), lgb.log_evaluation(0)])\n",
    "    lgb_val_prob  = lgb_model.predict(X_val_lgb)\n",
    "    lgb_test_prob = lgb_model.predict(X_test_lgb)\n",
    "    lgb_preds[days] = lgb_test_prob\n",
    "    print(f\"[LGB {days}d]  AUC={roc_auc_score(y_val,lgb_val_prob):.4f}  LogLoss={log_loss(y_val,lgb_val_prob):.4f}  PR-AUC={average_precision_score(y_val,lgb_val_prob):.4f}\")\n",
    "\n",
    "    # ── CatBoost ──\n",
    "    cat_model = CatBoostClassifier(\n",
    "        iterations=2000, loss_function='Logloss', eval_metric='Logloss',\n",
    "        cat_features=cat_feature_indices, early_stopping_rounds=50,\n",
    "        random_seed=42, verbose=0, **best_params[days]['cat']\n",
    "    )\n",
    "    cat_model.fit(X_tr_cat, y_tr, eval_set=(X_val_cat, y_val))\n",
    "    cat_val_prob  = cat_model.predict_proba(X_val_cat)[:, 1]\n",
    "    cat_test_prob = cat_model.predict_proba(X_test_cat)[:, 1]\n",
    "    cat_preds[days] = cat_test_prob\n",
    "    print(f\"[CAT {days}d]  AUC={roc_auc_score(y_val,cat_val_prob):.4f}  LogLoss={log_loss(y_val,cat_val_prob):.4f}  PR-AUC={average_precision_score(y_val,cat_val_prob):.4f}\")\n",
    "\n",
    "    # ── Ensemble (tuned weight) ──\n",
    "    best_w = best_weights[days]\n",
    "    ens_val_prob  = best_w * cat_val_prob  + (1-best_w) * lgb_val_prob\n",
    "    ens_test_prob = best_w * cat_test_prob + (1-best_w) * lgb_test_prob\n",
    "    ens_preds[days] = ens_test_prob\n",
    "    print(f\"[ENS {days}d]  AUC={roc_auc_score(y_val,ens_val_prob):.4f}  LogLoss={log_loss(y_val,ens_val_prob):.4f}  PR-AUC={average_precision_score(y_val,ens_val_prob):.4f}  CAT_w={best_w:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission.csv — 5621 rows\n",
      "          ID  Target_07_AUC  Target_90_AUC  Target_120_AUC  Target_07_LogLoss  \\\n",
      "0  ID_LEG1GM       0.006235       0.005746        0.013282           0.006235   \n",
      "1  ID_1UKOKW       0.005378       0.002327        0.011619           0.005378   \n",
      "2  ID_U5H2YK       0.040670       0.021337        0.031755           0.040670   \n",
      "\n",
      "   Target_90_LogLoss  Target_120_LogLoss  \n",
      "0           0.005746            0.013282  \n",
      "1           0.002327            0.011619  \n",
      "2           0.021337            0.031755  \n"
     ]
    }
   ],
   "source": [
    "# ── Submission ────────────────────────────────────────────────────────────────\n",
    "ss = pd.read_csv('/app/digicow/data/SampleSubmission.csv')[['ID']]\n",
    "\n",
    "for days in targets:\n",
    "    ss[f'Target_{days}_AUC']     = ens_preds[days]\n",
    "    ss[f'Target_{days}_LogLoss'] = ens_preds[days]\n",
    "\n",
    "col_order = ['ID',\n",
    "             'Target_07_AUC', 'Target_90_AUC', 'Target_120_AUC',\n",
    "             'Target_07_LogLoss', 'Target_90_LogLoss', 'Target_120_LogLoss']\n",
    "\n",
    "ss = ss[col_order]\n",
    "ss.to_csv('/app/digicow/submission.csv', index=False)\n",
    "print(f\"Saved submission.csv — {ss.shape[0]} rows\")\n",
    "print(ss.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LGB 07d]  AUC=0.8320  LogLoss=0.0671  PR-AUC=0.1443  Naive-PR=0.0106\n",
      "[CAT 07d]  AUC=0.9068  LogLoss=0.0644  PR-AUC=0.1757\n",
      "[ENS 07d]  AUC=0.9134  LogLoss=0.0632  PR-AUC=0.1916\n",
      "\n",
      "[LGB 90d]  AUC=0.7277  LogLoss=0.0951  PR-AUC=0.0572  Naive-PR=0.0160\n",
      "[CAT 90d]  AUC=0.9001  LogLoss=0.0644  PR-AUC=0.1650\n",
      "[ENS 90d]  AUC=0.8500  LogLoss=0.0779  PR-AUC=0.1334\n",
      "\n",
      "[LGB 120d]  AUC=0.8882  LogLoss=0.0751  PR-AUC=0.0905  Naive-PR=0.0237\n",
      "[CAT 120d]  AUC=0.9230  LogLoss=0.0629  PR-AUC=0.2002\n",
      "[ENS 120d]  AUC=0.9007  LogLoss=0.0679  PR-AUC=0.1825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## W/O tuning\n",
    "\n",
    "# LightGBM needs categoricals as 'category' dtype\n",
    "def prep_lgb(df):\n",
    "    X = df[feature_cols].copy()\n",
    "    for c in cat_cols:\n",
    "        X[c] = X[c].astype('category')\n",
    "    return X\n",
    "\n",
    "X_tr_lgb   = prep_lgb(df_tr)\n",
    "X_val_lgb  = prep_lgb(df_val)\n",
    "X_test_lgb = prep_lgb(test_df)\n",
    "\n",
    "# CatBoost takes raw strings — no encoding needed\n",
    "X_tr_cat   = df_tr[feature_cols].fillna('missing')\n",
    "X_val_cat  = df_val[feature_cols].fillna('missing')\n",
    "X_test_cat = test_df[feature_cols].fillna('missing')\n",
    "\n",
    "cat_feature_indices = [feature_cols.index(c) for c in cat_cols]\n",
    "\n",
    "targets = ['07', '90', '120']\n",
    "lgb_preds  = {}\n",
    "cat_preds  = {}\n",
    "xgb_preds = {}\n",
    "ens_preds  = {}\n",
    "\n",
    "\n",
    "# raise SystemExit\n",
    "\n",
    "# ── 5. Train per target ──────────────────────────────────────────────────────\n",
    "for days in targets:\n",
    "    col = f'adopted_within_{days}_days'\n",
    "    y_tr  = df_tr[col].values\n",
    "    y_val = df_val[col].values\n",
    "\n",
    "    prevalence = y_tr.mean()\n",
    "    scale_pos  = (1 - prevalence) / prevalence  # for imbalance\n",
    "\n",
    "    # ── LightGBM ──\n",
    "    lgb_tr  = lgb.Dataset(X_tr_lgb,  label=y_tr)\n",
    "    lgb_val = lgb.Dataset(X_val_lgb, label=y_val, reference=lgb_tr)\n",
    "\n",
    "    params = {\n",
    "        'objective':        'binary',\n",
    "        'metric':           ['binary_logloss', 'auc'],\n",
    "        'scale_pos_weight': scale_pos,\n",
    "        'learning_rate':    0.05,\n",
    "        'num_leaves':       31,\n",
    "        'min_child_samples': 20,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq':     5,\n",
    "        'verbose':          -1,\n",
    "        'seed':             42,\n",
    "    }\n",
    "    lgb_model = lgb.train(\n",
    "        params,\n",
    "        lgb_tr,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[lgb_val],\n",
    "        callbacks=[lgb.early_stopping(50, verbose=False), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    lgb_val_prob  = lgb_model.predict(X_val_lgb)\n",
    "    lgb_test_prob = lgb_model.predict(X_test_lgb)\n",
    "    lgb_preds[days] = lgb_test_prob\n",
    "\n",
    "    auc   = roc_auc_score(y_val, lgb_val_prob)\n",
    "    ll    = log_loss(y_val, lgb_val_prob)\n",
    "    prauc = average_precision_score(y_val, lgb_val_prob)\n",
    "    print(f\"[LGB {days}d]  AUC={auc:.4f}  LogLoss={ll:.4f}  PR-AUC={prauc:.4f}  Naive-PR={prevalence:.4f}\")\n",
    "\n",
    "    # ── CatBoost ──\n",
    "    cat_model = CatBoostClassifier(\n",
    "        iterations=1000,\n",
    "        learning_rate=0.03,\n",
    "        depth=6,\n",
    "        loss_function='Logloss',\n",
    "        eval_metric='Logloss',  # optimise for LogLoss (75% of score)\n",
    "        l2_leaf_reg=5,\n",
    "        cat_features=cat_feature_indices,\n",
    "        early_stopping_rounds=50,\n",
    "        random_seed=42,\n",
    "        verbose=0,\n",
    "    )\n",
    "    cat_model.fit(\n",
    "        X_tr_cat, y_tr,\n",
    "        eval_set=(X_val_cat, y_val),\n",
    "    )\n",
    "    cat_val_prob  = cat_model.predict_proba(X_val_cat)[:, 1]\n",
    "    cat_test_prob = cat_model.predict_proba(X_test_cat)[:, 1]\n",
    "    cat_preds[days] = cat_test_prob\n",
    "\n",
    "    auc   = roc_auc_score(y_val, cat_val_prob)\n",
    "    ll    = log_loss(y_val, cat_val_prob)\n",
    "    prauc = average_precision_score(y_val, cat_val_prob)\n",
    "    print(f\"[CAT {days}d]  AUC={auc:.4f}  LogLoss={ll:.4f}  PR-AUC={prauc:.4f}\")\n",
    "\n",
    "    \n",
    "    # ── Ensemble (simple average) ──\n",
    "    ens_val_prob  = (lgb_val_prob + cat_val_prob) / 2\n",
    "    ens_test_prob = (lgb_test_prob + cat_test_prob) / 2\n",
    "    ens_preds[days] = ens_test_prob\n",
    "\n",
    "    auc   = roc_auc_score(y_val, ens_val_prob)\n",
    "    ll    = log_loss(y_val, ens_val_prob)\n",
    "    prauc = average_precision_score(y_val, ens_val_prob)\n",
    "    print(f\"[ENS {days}d]  AUC={auc:.4f}  LogLoss={ll:.4f}  PR-AUC={prauc:.4f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# # ── 6. Submission ────────────────────────────────────────────────────────────\n",
    "# ss = pd.read_csv('SampleSubmission.csv')[['ID']]\n",
    "# for days in targets:\n",
    "#     ss[f'Target_{days}_AUC']     = ens_preds[days]\n",
    "#     ss[f'Target_{days}_LogLoss'] = ens_preds[days]\n",
    "\n",
    "# col_order = ['ID',\n",
    "#              'Target_07_AUC', 'Target_90_AUC', 'Target_120_AUC',\n",
    "#              'Target_07_LogLoss', 'Target_90_LogLoss', 'Target_120_LogLoss']\n",
    "# ss[col_order].to_csv('submission_ensemble.csv', index=False)\n",
    "# print(\"Saved submission_ensemble.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD w/ Prior concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 56,200 | Val: 2,218\n",
      "[LGB 07d]  AUC=0.9559  LogLoss=0.0614  PR-AUC=0.2416  Naive-PR=0.0140\n",
      "[CAT 07d]  AUC=0.9427  LogLoss=0.0542  PR-AUC=0.2755\n",
      "[ENS 07d]  AUC=0.9566  LogLoss=0.0532  PR-AUC=0.2601\n",
      "\n",
      "[LGB 90d]  AUC=0.9321  LogLoss=0.0779  PR-AUC=0.1605  Naive-PR=0.0304\n",
      "[CAT 90d]  AUC=0.9518  LogLoss=0.0566  PR-AUC=0.3277\n",
      "[ENS 90d]  AUC=0.9532  LogLoss=0.0640  PR-AUC=0.2799\n",
      "\n",
      "[LGB 120d]  AUC=0.9084  LogLoss=0.0883  PR-AUC=0.0823  Naive-PR=0.0419\n",
      "[CAT 120d]  AUC=0.9167  LogLoss=0.0637  PR-AUC=0.2977\n",
      "[ENS 120d]  AUC=0.9364  LogLoss=0.0739  PR-AUC=0.2973\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # OLD ---- adds prior to train\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# from sklearn.metrics import roc_auc_score, log_loss, average_precision_score\n",
    "# import lightgbm as lgb\n",
    "# from catboost import CatBoostClassifier\n",
    "# # import xgboost as xgb\n",
    "# import ast\n",
    "\n",
    "# # ── 1. Load ──────────────────────────────────────────────────────────────────\n",
    "# # train_df = pd.read_csv('Train.csv')\n",
    "# # test_df  = pd.read_csv('Test.csv')\n",
    "# train_df = pd.read_csv('/app/digicow/data/Train.csv')\n",
    "# prior_df = pd.read_csv('/app/digicow/data/Prior.csv')\n",
    "# test_df = pd.read_csv('/app/digicow/data/Test.csv')\n",
    "\n",
    "# # train_df = pd.concat([prior_df, train_df], ignore_index=True) # PRIOR\n",
    "\n",
    "# # raise SystemExit(\"Stopping here to check data.\")\n",
    "\n",
    "# # ── 2. Parse topics ──────────────────────────────────────────────────────────\n",
    "# def parse_topics(s):\n",
    "#     try:\n",
    "#         parsed = ast.literal_eval(s)\n",
    "#         if isinstance(parsed, list):\n",
    "#             flat = []\n",
    "#             for item in parsed:\n",
    "#                 if isinstance(item, list): flat.extend(item)\n",
    "#                 else: flat.append(item)\n",
    "#             return flat\n",
    "#     except:\n",
    "#         return []\n",
    "        \n",
    "# full_train = pd.concat([prior_df, train_df], ignore_index=True)\n",
    "\n",
    "# mlb = MultiLabelBinarizer()\n",
    "# topics_train = pd.DataFrame(\n",
    "#     # mlb.fit_transform(train_df['topics_list'].apply(parse_topics)),\n",
    "#     # columns=mlb.classes_, index=train_df.index\n",
    "#     mlb.fit_transform(full_train['topics_list'].apply(parse_topics)),\n",
    "#     columns=mlb.classes_, index=full_train.index\n",
    "# )\n",
    "# topics_test = pd.DataFrame(\n",
    "#     mlb.transform(test_df['topics_list'].apply(parse_topics)),\n",
    "#     columns=mlb.classes_, index=test_df.index\n",
    "# )\n",
    "# # train_df = pd.concat([train_df, topics_train], axis=1).drop(columns=['topics_list'])\n",
    "# full_train = pd.concat([full_train, topics_train], axis=1).drop(columns=['topics_list'])\n",
    "# test_df  = pd.concat([test_df,  topics_test],  axis=1).drop(columns=['topics_list'])\n",
    "\n",
    "# # raise SystemExit(\"Stopping here to check data.\")\n",
    "\n",
    "# # # ── 3. Temporal split ────────────────────────────────────────────────────────\n",
    "# # train_df['dt'] = pd.to_datetime(train_df['training_day'])\n",
    "# # test_df['dt']  = pd.to_datetime(test_df['training_day'])\n",
    "# # cutoff = pd.Timestamp('2025-03-01')\n",
    "\n",
    "# # df_tr  = train_df[train_df['dt'] < cutoff].copy()\n",
    "# # df_val = train_df[train_df['dt'] >= cutoff].copy()\n",
    "# # print(f\"Train: {len(df_tr):,} | Val: {len(df_val):,}\")\n",
    "\n",
    "# # ── 3. Temporal split ────────────────────────────────────────────────────────\n",
    "# # train_df['dt'] = pd.to_datetime(train_df['training_day'])\n",
    "# full_train['dt'] = pd.to_datetime(full_train['training_day'])\n",
    "# test_df['dt']  = pd.to_datetime(test_df['training_day'])\n",
    "# # prior_df['dt'] = pd.to_datetime(prior_df['training_day'])\n",
    "\n",
    "# prior_encoded = full_train.iloc[:len(prior_df)].copy()\n",
    "# train_encoded = full_train.iloc[len(prior_df):].copy()\n",
    "\n",
    "# # cutoff = pd.Timestamp('2025-03-01')\n",
    "# # df_val = train_df[train_df['dt'] >= cutoff].copy()\n",
    "# # df_tr  = pd.concat([prior_df, train_df[train_df['dt'] < cutoff]], ignore_index=True)\n",
    "# # print(f\"Train: {len(df_tr):,} | Val: {len(df_val):,}\")\n",
    "\n",
    "# cutoff = pd.Timestamp('2025-01-01')\n",
    "# df_val = train_encoded[train_encoded['dt'] >= cutoff].copy()\n",
    "# df_tr  = pd.concat([prior_encoded, train_encoded[train_encoded['dt'] < cutoff]], ignore_index=True)\n",
    "# print(f\"Train: {len(df_tr):,} | Val: {len(df_val):,}\")\n",
    "\n",
    "# # # ── 3b. Target encoding (computed from df_tr only, no leakage) ────────────\n",
    "# # targets = ['07', '90', '120']\n",
    "# # te_cols = []\n",
    "# # for days in targets:\n",
    "# #     col = f'adopted_within_{days}_days'\n",
    "# #     for grp in ['trainer', 'county', 'subcounty', 'ward']:\n",
    "# #         rate_map    = df_tr.groupby(grp)[col].mean()\n",
    "# #         global_mean = df_tr[col].mean()\n",
    "# #         col_name    = f'{grp}_adopt_{days}d'\n",
    "# #         df_tr[col_name]   = df_tr[grp].map(rate_map).fillna(global_mean)\n",
    "# #         df_val[col_name]  = df_val[grp].map(rate_map).fillna(global_mean)\n",
    "# #         test_df[col_name] = test_df[grp].map(rate_map).fillna(global_mean)\n",
    "# #         te_cols.append(col_name)\n",
    "\n",
    "# # raise SystemExit(\"Stopping here to check data.\")\n",
    "# # ── 4. Features ──────────────────────────────────────────────────────────────\n",
    "# cat_cols     = ['gender', 'registration', 'age', 'trainer', 'county', 'subcounty', 'ward']\n",
    "# topic_cols   = list(mlb.classes_)\n",
    "# numeric_cols = ['belong_to_cooperative', 'has_topic_trained_on'] + topic_cols #+ te_cols\n",
    "# feature_cols = cat_cols + numeric_cols\n",
    "\n",
    "# # LightGBM needs categoricals as 'category' dtype\n",
    "# def prep_lgb(df):\n",
    "#     X = df[feature_cols].copy()\n",
    "#     for c in cat_cols:\n",
    "#         X[c] = X[c].astype('category')\n",
    "#     return X\n",
    "\n",
    "# X_tr_lgb   = prep_lgb(df_tr)\n",
    "# X_val_lgb  = prep_lgb(df_val)\n",
    "# X_test_lgb = prep_lgb(test_df)\n",
    "\n",
    "# # CatBoost takes raw strings — no encoding needed\n",
    "# X_tr_cat   = df_tr[feature_cols].fillna('missing')\n",
    "# X_val_cat  = df_val[feature_cols].fillna('missing')\n",
    "# X_test_cat = test_df[feature_cols].fillna('missing')\n",
    "\n",
    "# cat_feature_indices = [feature_cols.index(c) for c in cat_cols]\n",
    "\n",
    "# targets = ['07', '90', '120']\n",
    "# lgb_preds  = {}\n",
    "# cat_preds  = {}\n",
    "# # xgb_preds = {}\n",
    "# ens_preds  = {}\n",
    "\n",
    "# # ── 5. Train per target ──────────────────────────────────────────────────────\n",
    "# for days in targets:\n",
    "#     col = f'adopted_within_{days}_days'\n",
    "#     y_tr  = df_tr[col].values\n",
    "#     y_val = df_val[col].values\n",
    "\n",
    "#     prevalence = y_tr.mean()\n",
    "#     scale_pos  = (1 - prevalence) / prevalence  # for imbalance\n",
    "\n",
    "#     # ── LightGBM ──\n",
    "#     lgb_tr  = lgb.Dataset(X_tr_lgb,  label=y_tr)\n",
    "#     lgb_val = lgb.Dataset(X_val_lgb, label=y_val, reference=lgb_tr)\n",
    "\n",
    "#     params = {\n",
    "#         'objective':        'binary',\n",
    "#         'metric':           ['binary_logloss', 'auc'],\n",
    "#         'scale_pos_weight': scale_pos,\n",
    "#         'learning_rate':    0.05,\n",
    "#         'num_leaves':       31,\n",
    "#         'min_child_samples': 20,\n",
    "#         'feature_fraction': 0.8,\n",
    "#         'bagging_fraction': 0.8,\n",
    "#         'bagging_freq':     5,\n",
    "#         'verbose':          -1,\n",
    "#         'seed':             42,\n",
    "#     }\n",
    "#     lgb_model = lgb.train(\n",
    "#         params,\n",
    "#         lgb_tr,\n",
    "#         num_boost_round=500,\n",
    "#         valid_sets=[lgb_val],\n",
    "#         callbacks=[lgb.early_stopping(50, verbose=False), lgb.log_evaluation(0)]\n",
    "#     )\n",
    "#     lgb_val_prob  = lgb_model.predict(X_val_lgb)\n",
    "#     lgb_test_prob = lgb_model.predict(X_test_lgb)\n",
    "#     lgb_preds[days] = lgb_test_prob\n",
    "\n",
    "#     auc   = roc_auc_score(y_val, lgb_val_prob)\n",
    "#     ll    = log_loss(y_val, lgb_val_prob)\n",
    "#     prauc = average_precision_score(y_val, lgb_val_prob)\n",
    "#     print(f\"[LGB {days}d]  AUC={auc:.4f}  LogLoss={ll:.4f}  PR-AUC={prauc:.4f}  Naive-PR={prevalence:.4f}\")\n",
    "\n",
    "#     # ── CatBoost ──\n",
    "#     cat_model = CatBoostClassifier(\n",
    "#         iterations=1000,\n",
    "#         learning_rate=0.03,\n",
    "#         depth=6,\n",
    "#         loss_function='Logloss',\n",
    "#         eval_metric='Logloss',  # optimise for LogLoss (75% of score)\n",
    "#         l2_leaf_reg=5,\n",
    "#         cat_features=cat_feature_indices,\n",
    "#         early_stopping_rounds=50,\n",
    "#         random_seed=42,\n",
    "#         verbose=0,\n",
    "#     )\n",
    "#     cat_model.fit(\n",
    "#         X_tr_cat, y_tr,\n",
    "#         eval_set=(X_val_cat, y_val),\n",
    "#     )\n",
    "#     cat_val_prob  = cat_model.predict_proba(X_val_cat)[:, 1]\n",
    "#     cat_test_prob = cat_model.predict_proba(X_test_cat)[:, 1]\n",
    "#     cat_preds[days] = cat_test_prob\n",
    "\n",
    "#     auc   = roc_auc_score(y_val, cat_val_prob)\n",
    "#     ll    = log_loss(y_val, cat_val_prob)\n",
    "#     prauc = average_precision_score(y_val, cat_val_prob)\n",
    "#     print(f\"[CAT {days}d]  AUC={auc:.4f}  LogLoss={ll:.4f}  PR-AUC={prauc:.4f}\")\n",
    "\n",
    "#     # # ── XGBoost ──\n",
    "#     # xgb_tr  = xgb.DMatrix(X_tr_lgb,  label=y_tr,  enable_categorical=True)\n",
    "#     # xgb_val = xgb.DMatrix(X_val_lgb, label=y_val, enable_categorical=True)\n",
    "#     # xgb_test = xgb.DMatrix(X_test_lgb,             enable_categorical=True)\n",
    "\n",
    "#     # xgb_params = {\n",
    "#     #     'objective':        'binary:logistic',\n",
    "#     #     'eval_metric':      ['logloss', 'auc'],\n",
    "#     #     'scale_pos_weight': scale_pos,\n",
    "#     #     'learning_rate':    0.05,\n",
    "#     #     'max_depth':        4,\n",
    "#     #     'subsample':        0.8,\n",
    "#     #     'colsample_bytree': 0.8,\n",
    "#     #     'min_child_weight': 20,\n",
    "#     #     'tree_method':      'hist',\n",
    "#     #     'device':           'cpu',\n",
    "#     #     'seed':             42,\n",
    "#     # }\n",
    "#     # xgb_model = xgb.train(\n",
    "#     #     xgb_params,\n",
    "#     #     xgb_tr,\n",
    "#     #     num_boost_round=500,\n",
    "#     #     evals=[(xgb_val, 'val')],\n",
    "#     #     early_stopping_rounds=50,\n",
    "#     #     verbose_eval=False,\n",
    "#     # )\n",
    "#     # xgb_val_prob  = xgb_model.predict(xgb_val)\n",
    "#     # xgb_test_prob = xgb_model.predict(xgb_test)\n",
    "#     # xgb_preds[days] = xgb_test_prob\n",
    "\n",
    "#     # auc   = roc_auc_score(y_val, xgb_val_prob)\n",
    "#     # ll    = log_loss(y_val, xgb_val_prob)\n",
    "#     # prauc = average_precision_score(y_val, xgb_val_prob)\n",
    "#     # print(f\"[XGB {days}d]  AUC={auc:.4f}  LogLoss={ll:.4f}  PR-AUC={prauc:.4f}\")\n",
    "\n",
    "\n",
    "    \n",
    "#     # ── Ensemble (simple average) ──\n",
    "#     ens_val_prob  = (lgb_val_prob + cat_val_prob) / 2\n",
    "#     ens_test_prob = (lgb_test_prob + cat_test_prob) / 2\n",
    "#     ens_preds[days] = ens_test_prob\n",
    "\n",
    "#     auc   = roc_auc_score(y_val, ens_val_prob)\n",
    "#     ll    = log_loss(y_val, ens_val_prob)\n",
    "#     prauc = average_precision_score(y_val, ens_val_prob)\n",
    "#     print(f\"[ENS {days}d]  AUC={auc:.4f}  LogLoss={ll:.4f}  PR-AUC={prauc:.4f}\")\n",
    "#     print()\n",
    "\n",
    "#     # # ── Ensemble (3-way average) ──\n",
    "#     # ens_val_prob  = (lgb_val_prob + cat_val_prob + xgb_val_prob) / 3\n",
    "#     # ens_test_prob = (lgb_test_prob + cat_test_prob + xgb_test_prob) / 3\n",
    "#     # ens_preds[days] = ens_test_prob\n",
    "\n",
    "#     # auc   = roc_auc_score(y_val, ens_val_prob)\n",
    "#     # ll    = log_loss(y_val, ens_val_prob)\n",
    "#     # prauc = average_precision_score(y_val, ens_val_prob)\n",
    "#     # print(f\"[ENS {days}d]  AUC={auc:.4f}  LogLoss={ll:.4f}  PR-AUC={prauc:.4f}\")\n",
    "#     # print()\n",
    "\n",
    "# # # ── 6. Submission ────────────────────────────────────────────────────────────\n",
    "# # ss = pd.read_csv('SampleSubmission.csv')[['ID']]\n",
    "# # for days in targets:\n",
    "# #     ss[f'Target_{days}_AUC']     = ens_preds[days]\n",
    "# #     ss[f'Target_{days}_LogLoss'] = ens_preds[days]\n",
    "\n",
    "# # col_order = ['ID',\n",
    "# #              'Target_07_AUC', 'Target_90_AUC', 'Target_120_AUC',\n",
    "# #              'Target_07_LogLoss', 'Target_90_LogLoss', 'Target_120_LogLoss']\n",
    "# # ss[col_order].to_csv('submission_ensemble.csv', index=False)\n",
    "# # print(\"Saved submission_ensemble.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score, log_loss, average_precision_score\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "import optuna\n",
    "import ast\n",
    "import json\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# ── 1. Load ───────────────────────────────────────────────────────────────────\n",
    "train_df = pd.read_csv('/app/digicow/data/Train.csv')\n",
    "prior_df = pd.read_csv('/app/digicow/data/Prior.csv')\n",
    "test_df  = pd.read_csv('/app/digicow/data/Test.csv')\n",
    "\n",
    "# ── 2. Parse topics ───────────────────────────────────────────────────────────\n",
    "def parse_topics(s):\n",
    "    try:\n",
    "        parsed = ast.literal_eval(s)\n",
    "        if isinstance(parsed, list):\n",
    "            flat = []\n",
    "            for item in parsed:\n",
    "                if isinstance(item, list): flat.extend(item)\n",
    "                else: flat.append(item)\n",
    "            return flat\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "full_train = pd.concat([prior_df, train_df], ignore_index=True)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "topics_train = pd.DataFrame(\n",
    "    mlb.fit_transform(full_train['topics_list'].apply(parse_topics)),\n",
    "    columns=mlb.classes_, index=full_train.index\n",
    ")\n",
    "topics_test = pd.DataFrame(\n",
    "    mlb.transform(test_df['topics_list'].apply(parse_topics)),\n",
    "    columns=mlb.classes_, index=test_df.index\n",
    ")\n",
    "full_train = pd.concat([full_train, topics_train], axis=1).drop(columns=['topics_list'])\n",
    "test_df    = pd.concat([test_df, topics_test],     axis=1).drop(columns=['topics_list'])\n",
    "\n",
    "# ── 3. Temporal split ─────────────────────────────────────────────────────────\n",
    "full_train['dt'] = pd.to_datetime(full_train['training_day'])\n",
    "test_df['dt']    = pd.to_datetime(test_df['training_day'])\n",
    "\n",
    "prior_encoded = full_train.iloc[:len(prior_df)].copy()\n",
    "train_encoded = full_train.iloc[len(prior_df):].copy()\n",
    "\n",
    "cutoff = pd.Timestamp('2025-03-01')\n",
    "df_val = train_encoded[train_encoded['dt'] >= cutoff].copy()\n",
    "df_tr  = pd.concat([prior_encoded, train_encoded[train_encoded['dt'] < cutoff]], ignore_index=True)\n",
    "print(f\"Train: {len(df_tr):,} | Val: {len(df_val):,}\")\n",
    "\n",
    "# ── 4. Features ───────────────────────────────────────────────────────────────\n",
    "cat_cols     = ['gender', 'registration', 'age', 'trainer', 'county', 'subcounty', 'ward']\n",
    "topic_cols   = list(mlb.classes_)\n",
    "numeric_cols = ['belong_to_cooperative', 'has_topic_trained_on'] + topic_cols\n",
    "feature_cols = cat_cols + numeric_cols\n",
    "\n",
    "def prep_lgb(df):\n",
    "    X = df[feature_cols].copy()\n",
    "    for c in cat_cols:\n",
    "        X[c] = X[c].astype('category')\n",
    "    return X\n",
    "\n",
    "X_tr_lgb   = prep_lgb(df_tr)\n",
    "X_val_lgb  = prep_lgb(df_val)\n",
    "X_test_lgb = prep_lgb(test_df)\n",
    "\n",
    "X_tr_cat   = df_tr[feature_cols].fillna('missing')\n",
    "X_val_cat  = df_val[feature_cols].fillna('missing')\n",
    "X_test_cat = test_df[feature_cols].fillna('missing')\n",
    "\n",
    "cat_feature_indices = [feature_cols.index(c) for c in cat_cols]\n",
    "targets = ['07', '90', '120']\n",
    "\n",
    "# ── 5. Optuna tuning ──────────────────────────────────────────────────────────\n",
    "def tune_lgb(X_tr, y_tr, X_val, y_val, scale_pos):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'objective':         'binary',\n",
    "            'metric':            'binary_logloss',\n",
    "            'scale_pos_weight':  scale_pos,\n",
    "            'verbose':           -1,\n",
    "            'seed':              42,\n",
    "            'learning_rate':     trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "            'num_leaves':        trial.suggest_int('num_leaves', 16, 128),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "            'feature_fraction':  trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "            'bagging_fraction':  trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "            'bagging_freq':      trial.suggest_int('bagging_freq', 1, 10),\n",
    "            'reg_alpha':         trial.suggest_float('reg_alpha', 1e-4, 10.0, log=True),\n",
    "            'reg_lambda':        trial.suggest_float('reg_lambda', 1e-4, 10.0, log=True),\n",
    "        }\n",
    "        ds_tr  = lgb.Dataset(X_tr, label=y_tr)\n",
    "        ds_val = lgb.Dataset(X_val, label=y_val, reference=ds_tr)\n",
    "        model = lgb.train(\n",
    "            params, ds_tr,\n",
    "            num_boost_round=500,\n",
    "            valid_sets=[ds_val],\n",
    "            callbacks=[lgb.early_stopping(50, verbose=False), lgb.log_evaluation(0)]\n",
    "        )\n",
    "        return log_loss(y_val, model.predict(X_val))\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "    return study.best_params\n",
    "\n",
    "def tune_cat(X_tr, y_tr, X_val, y_val, cat_feature_indices):\n",
    "    def objective(trial):\n",
    "        model = CatBoostClassifier(\n",
    "            iterations=1000, random_seed=42, verbose=0,\n",
    "            cat_features=cat_feature_indices,\n",
    "            early_stopping_rounds=50,\n",
    "            loss_function='Logloss', eval_metric='Logloss',\n",
    "            learning_rate=      trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "            depth=              trial.suggest_int('depth', 4, 8),\n",
    "            l2_leaf_reg=        trial.suggest_float('l2_leaf_reg', 1.0, 20.0),\n",
    "            bagging_temperature=trial.suggest_float('bagging_temperature', 0.0, 2.0),\n",
    "            random_strength=    trial.suggest_float('random_strength', 0.0, 2.0),\n",
    "            border_count=       trial.suggest_int('border_count', 32, 255),\n",
    "        )\n",
    "        model.fit(X_tr, y_tr, eval_set=(X_val, y_val))\n",
    "        return log_loss(y_val, model.predict_proba(X_val)[:, 1])\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
    "    return study.best_params\n",
    "\n",
    "best_params = {}\n",
    "for days in targets:\n",
    "    col = f'adopted_within_{days}_days'\n",
    "    y_tr  = df_tr[col].values\n",
    "    y_val = df_val[col].values\n",
    "    prevalence = y_tr.mean()\n",
    "    scale_pos  = (1 - prevalence) / prevalence\n",
    "\n",
    "    print(f\"\\n── Tuning {days}d ──\")\n",
    "    lgb_best = tune_lgb(X_tr_lgb, y_tr, X_val_lgb, y_val, scale_pos)\n",
    "    cat_best = tune_cat(X_tr_cat, y_tr, X_val_cat, y_val, cat_feature_indices)\n",
    "    best_params[days] = {'lgb': lgb_best, 'cat': cat_best}\n",
    "    print(f\"LGB best: {lgb_best}\")\n",
    "    print(f\"CAT best: {cat_best}\")\n",
    "\n",
    "# Save\n",
    "with open('/app/digicow/best_params_concat.json', 'w') as f:\n",
    "    json.dump({'params': best_params, 'weights': best_weights}, f, indent=2)\n",
    "print(\"Saved best_params_concat.json\")\n",
    "\n",
    "# Load\n",
    "with open('/app/digicow/best_params_concat.json') as f:\n",
    "    saved = json.load(f)\n",
    "best_params  = saved['params']\n",
    "best_weights = saved['weights']\n",
    "\n",
    "# ── 6. Train with tuned params ────────────────────────────────────────────────\n",
    "lgb_preds = {}\n",
    "cat_preds = {}\n",
    "ens_preds = {}\n",
    "\n",
    "for days in targets:\n",
    "    col = f'adopted_within_{days}_days'\n",
    "    y_tr  = df_tr[col].values\n",
    "    y_val = df_val[col].values\n",
    "    prevalence = y_tr.mean()\n",
    "    scale_pos  = (1 - prevalence) / prevalence\n",
    "\n",
    "    # ── LightGBM ──\n",
    "    lgb_tr     = lgb.Dataset(X_tr_lgb, label=y_tr)\n",
    "    lgb_val_ds = lgb.Dataset(X_val_lgb, label=y_val, reference=lgb_tr)\n",
    "    params = {\n",
    "        'objective': 'binary', 'metric': ['binary_logloss', 'auc'],\n",
    "        'scale_pos_weight': scale_pos, 'verbose': -1, 'seed': 42,\n",
    "        **best_params[days]['lgb']\n",
    "    }\n",
    "    lgb_model = lgb.train(\n",
    "        params, lgb_tr,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[lgb_val_ds],\n",
    "        callbacks=[lgb.early_stopping(50, verbose=False), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    lgb_val_prob  = lgb_model.predict(X_val_lgb)\n",
    "    lgb_test_prob = lgb_model.predict(X_test_lgb)\n",
    "    lgb_preds[days] = lgb_test_prob\n",
    "    print(f\"[LGB {days}d]  AUC={roc_auc_score(y_val, lgb_val_prob):.4f}  LogLoss={log_loss(y_val, lgb_val_prob):.4f}\")\n",
    "\n",
    "    # ── CatBoost ──\n",
    "    cat_model = CatBoostClassifier(\n",
    "        iterations=1000, loss_function='Logloss', eval_metric='Logloss',\n",
    "        cat_features=cat_feature_indices, early_stopping_rounds=50,\n",
    "        random_seed=42, verbose=0,\n",
    "        **best_params[days]['cat']\n",
    "    )\n",
    "    cat_model.fit(X_tr_cat, y_tr, eval_set=(X_val_cat, y_val))\n",
    "    cat_val_prob  = cat_model.predict_proba(X_val_cat)[:, 1]\n",
    "    cat_test_prob = cat_model.predict_proba(X_test_cat)[:, 1]\n",
    "    cat_preds[days] = cat_test_prob\n",
    "    print(f\"[CAT {days}d]  AUC={roc_auc_score(y_val, cat_val_prob):.4f}  LogLoss={log_loss(y_val, cat_val_prob):.4f}\")\n",
    "\n",
    "    # ── Ensemble ──\n",
    "    ens_val_prob  = (lgb_val_prob + cat_val_prob) / 2\n",
    "    ens_test_prob = (lgb_test_prob + cat_test_prob) / 2\n",
    "    ens_preds[days] = ens_test_prob\n",
    "    print(f\"[ENS {days}d]  AUC={roc_auc_score(y_val, ens_val_prob):.4f}  LogLoss={log_loss(y_val, ens_val_prob):.4f}\\n\")\n",
    "\n",
    "# # ── 7. Submission ─────────────────────────────────────────────────────────────\n",
    "# ss = pd.read_csv('/app/digicow/data/SampleSubmission.csv')[['ID']]\n",
    "# for days in targets:\n",
    "#     ss[f'Target_{days}_AUC']     = ens_preds[days]\n",
    "#     ss[f'Target_{days}_LogLoss'] = ens_preds[days]\n",
    "\n",
    "# col_order = ['ID',\n",
    "#              'Target_07_AUC', 'Target_90_AUC', 'Target_120_AUC',\n",
    "#              'Target_07_LogLoss', 'Target_90_LogLoss', 'Target_120_LogLoss']\n",
    "# ss[col_order].to_csv('/app/digicow/submission_tuned.csv', index=False)\n",
    "# print(\"Saved submission_tuned.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
